# Toxicity-Classifier
The toxicity model detects whether text contains toxic context. More about this model can be found there: https://github.com/tensorflow/tfjs-models/tree/master/toxicity . We will use the pre-trained Toxicity model to detect whether a given piece of text contains toxic content such as threatening language, insults, obscenities, identity-based hate, or sexually explicit language. 
